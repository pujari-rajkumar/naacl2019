{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from itertools import izip\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/homes/rpujari/scratch/naacl2019/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from semeval_pyscripts import text_feature_extraction\n",
    "from semeval_pyscripts import text_utils\n",
    "from semeval_pyscripts import amr_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(fp):\n",
    "    ret_data = []\n",
    "    d_lines = open(fp, 'r').read().split('\\n')\n",
    "    p_done = []\n",
    "    q_done = []\n",
    "    qps = []\n",
    "    answers = []\n",
    "    for d_line in d_lines:\n",
    "        if d_line.strip() != '':\n",
    "            d_dict = json.loads(d_line)\n",
    "            p_id, q_id, c_id = (int(x) for x in d_dict['id'].split('_'))\n",
    "            \n",
    "            if p_id not in p_done:\n",
    "                if len(answers) != 0:\n",
    "                    qps.append((q_doc, answers, 'text'))\n",
    "                if len(qps) != 0:\n",
    "                    ret_data.append((p_doc, qps))\n",
    "                qps = []\n",
    "                q_done = []\n",
    "                answers = []\n",
    "                p_sents = nltk.sent_tokenize(d_dict['d_words'])\n",
    "                p_doc = []\n",
    "                for p_sent in p_sents:\n",
    "                    if type(p_sent) == type('string'):\n",
    "                        p_doc.append(text_utils.spacy_nlp(unicode(p_sent, errors='ignore')))\n",
    "                    else:\n",
    "                        p_doc.append(text_utils.spacy_nlp(p_sent))\n",
    "                p_done.append(p_id)\n",
    "            \n",
    "            if q_id not in q_done:\n",
    "                if len(answers) != 0:\n",
    "                    qps.append((q_doc, answers, 'text'))\n",
    "                answers = []\n",
    "                q = d_dict['q_words']\n",
    "                if type(q) == type('string'):\n",
    "                    q_doc = text_utils.spacy_nlp(unicode(q, errors='ignore'))\n",
    "                else:\n",
    "                    q_doc = text_utils.spacy_nlp(q)\n",
    "                q_done.append(q_id)\n",
    "            \n",
    "            a = d_dict['c_words']\n",
    "            if type(a) == type('string'):\n",
    "                a_doc = text_utils.spacy_nlp(unicode(a, errors='ignore'))\n",
    "            else:\n",
    "                a_doc = text_utils.spacy_nlp(a)\n",
    "            if d_dict['label'] == 1:\n",
    "                at = 'True'\n",
    "            else:\n",
    "                at = 'False'\n",
    "            answers.append((a_doc, at, d_dict['id']))\n",
    "    if len(answers) != 0:\n",
    "        qps.append((q_doc, answers, 'text'))\n",
    "    if len(qps) != 0:\n",
    "        ret_data.append((p_doc, qps))\n",
    "    return ret_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multirc_train = read_data(filepath + 'data/multirc/multirc-train-processed.json')\n",
    "multirc_dev = read_data( filepath + 'data/multirc/multirc-dev-processed.json')\n",
    "multirc_test = read_data(filepath + 'data/multirc/dev_83-fixedIds-processed.json')\n",
    "multirc_train_full = json.load(open(filepath + 'data/multirc/train_456-fixedIds.json', 'r'))\n",
    "multirc_test_orig = json.load(open(filepath + 'data/multirc/dev_83-fixedIds.json', 'r'))\n",
    "train_paras, dev_paras = pickle.load(open(filepath + 'data/multirc/train_dev_split.pkl'))\n",
    "multirc_dev_orig = {}\n",
    "multirc_dev_orig['data'] = [multirc_train_full['data'][x] for x in dev_paras]\n",
    "multirc_train_orig = {}\n",
    "multirc_train_orig['data'] = [multirc_train_full['data'][x] for x in train_paras]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess YF and generate YF results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_results(data, data_orig, yf_res):\n",
    "    with open(filepath + 'tm_dev_res.json', 'w') as outfile:\n",
    "        p_num = 0\n",
    "        dev_res = []\n",
    "        for para in data:\n",
    "            q_num = 0\n",
    "            pid = data_orig['data'][p_num]['id']\n",
    "            for qp in para[1]:\n",
    "                q_dict = {}\n",
    "                q_dict['pid'] = pid\n",
    "                q_dict['qid'] = str(qp[1][0][2].split('_')[1])\n",
    "                answers = []\n",
    "                for ans in qp[1]:\n",
    "                    answers.append(yf_res[ans[2]])\n",
    "                q_dict['scores'] = answers\n",
    "                q_num += 1\n",
    "                dev_res.append(q_dict)\n",
    "            p_num += 1\n",
    "        outfile.write(json.dumps(dev_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id_dict = {}\n",
    "yf_train_res = {}\n",
    "used_qids = {}\n",
    "qid = 0\n",
    "cid = 0\n",
    "with open(filepath + 'results/multirc_train_res', 'r') as if1:\n",
    "    with open(filepath + 'DRaiL/examples/multirc/train_qids.txt', 'w') as of1:\n",
    "        with open(filepath + 'DRaiL/examples/multirc/train_cids.txt', 'w') as of2:\n",
    "            with open(filepath + 'DRaiL/examples/multirc/train_probs.json', 'w') as of3:\n",
    "                with open(filepath + 'DRaiL/examples/multirc/train_preds.txt', 'w') as of4:\n",
    "                    with open(filepath + 'DRaiL/examples/multirc/train_in_ques.txt', 'w') as of5:   \n",
    "                        f_lines = if1.read().split('\\n')\n",
    "                        prob_dict = {}\n",
    "                        for f_line in f_lines:\n",
    "                            cols = f_line.split('\\t')\n",
    "                            if len(cols) == 2 and int(cols[0].strip().split('_')[0]) in train_paras:\n",
    "                                #Find / Assign QID\n",
    "                                qnum = '_'.join(cols[0].strip().split('_')[:2])\n",
    "                                if qnum not in used_qids.keys():\n",
    "                                    used_qids[qnum] = str(qid)\n",
    "                                    qid1 = str(qid)\n",
    "                                    qid += 1\n",
    "                                    of1.write(qid1 + '\\n')\n",
    "                                else:\n",
    "                                    qid1 = str(used_qids[qnum])\n",
    "                                    \n",
    "                                #Assign CID\n",
    "                                train_id_dict[cols[0].strip()] = cid\n",
    "                                id1 = str(cid)\n",
    "                                cid += 1\n",
    "                                of2.write(id1 + '\\n')\n",
    "                                of5.write(qid1 + '\\t' + id1 + '\\n')\n",
    "                                    \n",
    "                                #Write prediction and probability\n",
    "                                if float(cols[1].strip()) > 0.5:\n",
    "                                    yf_train_res[cols[0].strip()] = 1\n",
    "                                else:\n",
    "                                    yf_train_res[cols[0].strip()] = 0\n",
    "                                prob_dict[int(id1)] = float(cols[1].strip())\n",
    "                                if float(cols[1].strip()) >= 0.5:\n",
    "                                    of4.write(id1 + '\\t1\\n')\n",
    "                                else:\n",
    "                                    of4.write(id1 + '\\t0\\n')\n",
    "                        \n",
    "                        #Dump prob scores           \n",
    "                        of3.write(json.dumps(prob_dict))\n",
    "                        \n",
    "                        #Dump train answer IDs\n",
    "                        with open(filepath + 'DRaiL/examples/multirc/train_id_dict.json', 'w') as of6:\n",
    "                            of6.write(json.dumps(train_id_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_id_dict = {}\n",
    "yf_dev_res = {}\n",
    "used_qids = {}\n",
    "qid = 0\n",
    "cid = 0\n",
    "with open(filepath + 'results/multirc_train_res', 'r') as if1:\n",
    "    with open(filepath + 'DRaiL/examples/multirc/dev_qids.txt', 'w') as of1:\n",
    "        with open(filepath + 'DRaiL/examples/multirc/dev_cids.txt', 'w') as of2:\n",
    "            with open(filepath + 'DRaiL/examples/multirc/dev_probs.json', 'w') as of3:\n",
    "                with open(filepath + 'DRaiL/examples/multirc/dev_preds.txt', 'w') as of4:\n",
    "                    with open(filepath + 'DRaiL/examples/multirc/dev_in_ques.txt', 'w') as of5:   \n",
    "                        f_lines = if1.read().split('\\n')\n",
    "                        prob_dict = {}\n",
    "                        for f_line in f_lines:\n",
    "                            cols = f_line.split('\\t')\n",
    "                            if len(cols) == 2 and int(cols[0].strip().split('_')[0]) in dev_paras:\n",
    "                                #Find / Assign QID\n",
    "                                qnum = '_'.join(cols[0].strip().split('_')[:2])\n",
    "                                if qnum not in used_qids.keys():\n",
    "                                    used_qids[qnum] = str(qid)\n",
    "                                    qid1 = str(qid)\n",
    "                                    qid += 1\n",
    "                                    of1.write(qid1 + '\\n')\n",
    "                                else:\n",
    "                                    qid1 = str(used_qids[qnum])\n",
    "                                    \n",
    "                                #Assign CID\n",
    "                                dev_id_dict[cols[0].strip()] = cid\n",
    "                                id1 = str(cid)\n",
    "                                cid += 1\n",
    "                                of2.write(id1 + '\\n')\n",
    "                                of5.write(qid1 + '\\t' + id1 + '\\n')\n",
    "                                    \n",
    "                                #Write prediction and probability\n",
    "                                if float(cols[1].strip()) > 0.5:\n",
    "                                    yf_dev_res[cols[0].strip()] = 1\n",
    "                                else:\n",
    "                                    yf_dev_res[cols[0].strip()] = 0\n",
    "                                prob_dict[int(id1)] = float(cols[1].strip())\n",
    "                                if float(cols[1].strip()) >= 0.5:\n",
    "                                    of4.write(id1 + '\\t1\\n')\n",
    "                                else:\n",
    "                                    of4.write(id1 + '\\t0\\n')\n",
    "                        \n",
    "                        #Dump prob scores           \n",
    "                        of3.write(json.dumps(prob_dict))\n",
    "                        \n",
    "                        #Dump train answer IDs\n",
    "                        with open(filepath + 'DRaiL/examples/multirc/dev_id_dict.json', 'w') as of6:\n",
    "                            of6.write(json.dumps(dev_id_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id_dict = {}\n",
    "yf_test_res = {}\n",
    "used_qids = {}\n",
    "qid = 0\n",
    "cid = 0\n",
    "with open(filepath + 'results/multirc_test_res', 'r') as if1:\n",
    "    with open(filepath + 'DRaiL/examples/multirc/test_qids.txt', 'w') as of1:\n",
    "        with open(filepath + 'DRaiL/examples/multirc/test_cids.txt', 'w') as of2:\n",
    "            with open(filepath + 'DRaiL/examples/multirc/test_probs.json', 'w') as of3:\n",
    "                with open(filepath + 'DRaiL/examples/multirc/test_preds.txt', 'w') as of4:\n",
    "                    with open(filepath + 'DRaiL/examples/multirc/test_in_ques.txt', 'w') as of5:   \n",
    "                        f_lines = if1.read().split('\\n')\n",
    "                        prob_dict = {}\n",
    "                        for f_line in f_lines:\n",
    "                            cols = f_line.split('\\t')\n",
    "                            #print cols\n",
    "                            if len(cols) == 2 and int(cols[0].strip().split('_')[0]) > -1:\n",
    "                                #Find / Assign QID\n",
    "                                qnum = '_'.join(cols[0].strip().split('_')[:2])\n",
    "                                if qnum not in used_qids.keys():\n",
    "                                    used_qids[qnum] = str(qid)\n",
    "                                    qid1 = str(qid)\n",
    "                                    qid += 1\n",
    "                                    of1.write(qid1 + '\\n')\n",
    "                                else:\n",
    "                                    qid1 = str(used_qids[qnum])\n",
    "                                    \n",
    "                                #Assign CID\n",
    "                                test_id_dict[cols[0].strip()] = cid\n",
    "                                id1 = str(cid)\n",
    "                                cid += 1\n",
    "                                of2.write(id1 + '\\n')\n",
    "                                of5.write(qid1 + '\\t' + id1 + '\\n')\n",
    "                                    \n",
    "                                #Write prediction and probability\n",
    "                                if float(cols[1].strip()) >= 0.5:\n",
    "                                    yf_test_res[cols[0].strip()] = 1\n",
    "                                else:\n",
    "                                    yf_test_res[cols[0].strip()] = 0\n",
    "                                prob_dict[int(id1)] = float(cols[1].strip())\n",
    "                                if float(cols[1].strip()) >= 0.5:\n",
    "                                    of4.write(id1 + '\\t1\\n')\n",
    "                                else:\n",
    "                                    of4.write(id1 + '\\t0\\n')\n",
    "                        #Dump prob scores           \n",
    "                        of3.write(json.dumps(prob_dict))\n",
    "                        \n",
    "                        #Dump train answer IDs\n",
    "                        with open(filepath + 'DRaiL/examples/multirc/test_id_dict.json', 'w') as of6:\n",
    "                            of6.write(json.dumps(test_id_dict))\n",
    "\n",
    "with open(filepath + 'DRaiL/examples/multirc/test_pids.txt', 'w') as of0:\n",
    "    for i, para in enumerate(multirc_test):\n",
    "        of0.write(str(i) + '\\n')\n",
    "    \n",
    "with open(filepath + 'DRaiL/examples/multirc/test_in_para.txt', 'w') as of6:\n",
    "    for key in test_id_dict:\n",
    "        pid = int(key.split('_')[0])\n",
    "        of6.write(str(pid) + '\\t' + str(test_id_dict[key]) + '\\n')\n",
    "    \n",
    "with open(filepath + 'DRaiL/examples/multirc/pairs.txt', 'w') as of7:\n",
    "    for k1 in test_id_dict:\n",
    "        of7.write(str(test_id_dict[k1]) + '\\t' + str(test_id_dict[k1]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_results(multirc_test, multirc_test_orig, yf_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25818\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "train_labels = {}\n",
    "src_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/src-train.txt', 'w', 'utf-8')\n",
    "targ_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/targ-train.txt', 'w', 'utf-8')\n",
    "ques_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/ques-train.txt', 'w', 'utf-8')\n",
    "label_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/label-train.txt', 'w', 'utf-8')\n",
    "id_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/ids-train.txt', 'w', 'utf-8')\n",
    "for para in multirc_train:\n",
    "    for ques in para[1]:\n",
    "        answers = ques[1]\n",
    "        ques = ' '.join(nltk.word_tokenize(ques[0].text.replace('\"', '')))\n",
    "        for i, a1 in enumerate(answers):\n",
    "            try:\n",
    "                hyp_l = a1[1]\n",
    "                hyp_id = train_id_dict[a1[2]]\n",
    "                hyp = ' '.join(nltk.word_tokenize(a1[0].text.replace('\"', '')))\n",
    "                train_labels[hyp_id] = hyp_l\n",
    "                for j, a2 in enumerate(answers):\n",
    "                    if i != j:\n",
    "                        prem_l = a2[1]\n",
    "                        prem_id = train_id_dict[a2[2]]\n",
    "                        prem = ' '.join(nltk.word_tokenize(a2[0].text.replace('\"', '')))\n",
    "                        src_writer.write(str(hyp_id) + '\\t' + hyp + '\\n')\n",
    "                        targ_writer.write(str(prem_id) + '\\t' + prem + '\\n')\n",
    "                        ques_writer.write(ques.encode(errors='ignore') + '\\n')\n",
    "                        label_writer.write('neutral\\n')\n",
    "                        id_writer.write(str(hyp_id) + '\\t' + str(prem_id) + '\\t' + str(hyp_l) + '\\t' + str(prem_l) + '\\n')\n",
    "            except KeyError:\n",
    "                pass\n",
    "src_writer.close()\n",
    "targ_writer.close()\n",
    "ques_writer.close()\n",
    "label_writer.close()\n",
    "id_writer.close()\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "dev_labels = {}\n",
    "src_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/src-dev.txt', 'w', 'utf-8')\n",
    "targ_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/targ-dev.txt', 'w', 'utf-8')\n",
    "ques_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/ques-dev.txt', 'w', 'utf-8')\n",
    "label_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/label-dev.txt', 'w', 'utf-8')\n",
    "id_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/ids-dev.txt', 'w', 'utf-8')\n",
    "for para in multirc_dev:\n",
    "    for ques in para[1]:\n",
    "        answers = ques[1]\n",
    "        ques = ' '.join(nltk.word_tokenize(ques[0].text.replace('\"', '')))\n",
    "        for i, a1 in enumerate(answers):\n",
    "            hyp_id = dev_id_dict[a1[2]]\n",
    "            hyp_l = a1[1]\n",
    "            hyp = ' '.join(nltk.word_tokenize(a1[0].text.replace('\"', '')))\n",
    "            dev_labels[hyp_id] = hyp_l\n",
    "            for j, a2 in enumerate(answers):\n",
    "                if i != j:\n",
    "                    prem_id = dev_id_dict[a2[2]]\n",
    "                    prem_l = a2[1]\n",
    "                    prem = ' '.join(nltk.word_tokenize(a2[0].text.replace('\"', '')))\n",
    "                    src_writer.write(str(hyp_id) + '\\t' + hyp + '\\n')\n",
    "                    targ_writer.write(str(prem_id) + '\\t' + prem + '\\n')\n",
    "                    ques_writer.write(ques.encode(errors='ignore') + '\\n')\n",
    "                    label_writer.write('neutral\\n')\n",
    "                    id_writer.write(str(hyp_id) + '\\t' + str(prem_id) + '\\t' + str(hyp_l) + '\\t' + str(prem_l) + '\\n')\n",
    "src_writer.close()\n",
    "targ_writer.close()\n",
    "ques_writer.close()\n",
    "label_writer.close()\n",
    "id_writer.close()\n",
    "print(len(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4848\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "test_labels = {}\n",
    "src_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/src-test.txt', 'w', 'utf-8')\n",
    "targ_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/targ-test.txt', 'w', 'utf-8')\n",
    "ques_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/ques-test.txt', 'w', 'utf-8')\n",
    "label_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/label-test.txt', 'w', 'utf-8')\n",
    "id_writer = codecs.open(filepath + 'nli/preprocess/proc_multirc/ids-test.txt', 'w', 'utf-8')\n",
    "for para in multirc_test:\n",
    "    for ques in para[1]:\n",
    "        answers = ques[1]\n",
    "        ques = ' '.join(nltk.word_tokenize(ques[0].text.replace('\"', '')))\n",
    "        for i, a1 in enumerate(answers):\n",
    "            hyp_id = test_id_dict[a1[2]]\n",
    "            hyp_l = a1[1]\n",
    "            hyp = ' '.join(nltk.word_tokenize(a1[0].text.replace('\"', '')))\n",
    "            test_labels[hyp_id] = hyp_l\n",
    "            for j, a2 in enumerate(answers):\n",
    "                if i != j:\n",
    "                    prem_id = test_id_dict[a2[2]]\n",
    "                    prem_l = a2[1]\n",
    "                    prem = ' '.join(nltk.word_tokenize(a2[0].text.replace('\"', '')))\n",
    "                    src_writer.write(str(hyp_id) + '\\t' + hyp + '\\n')\n",
    "                    targ_writer.write(str(prem_id) + '\\t' + prem + '\\n')\n",
    "                    ques_writer.write(ques.encode(errors='ignore') + '\\n')\n",
    "                    label_writer.write('neutral\\n')\n",
    "                    id_writer.write(str(hyp_id) + '\\t' + str(prem_id) + '\\t' + str(hyp_l) + '\\t' + str(prem_l) + '\\n')\n",
    "src_writer.close()\n",
    "targ_writer.close()\n",
    "ques_writer.close()\n",
    "label_writer.close()\n",
    "id_writer.close()\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preprocess NLI and generate NLI results here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22265\n"
     ]
    }
   ],
   "source": [
    "train_edict = {}\n",
    "for l1 in open(filepath + 'results/multirc_train_nli', 'r').read().split('\\n'):\n",
    "    c1 = l1.split('\\t')\n",
    "    if len(c1) == 4:\n",
    "        try:\n",
    "            train_edict[int(c1[0])][int(c1[3])][int(c1[1])] = float(c1[2])\n",
    "        except KeyError:\n",
    "            train_edict[int(c1[0])] = ({},{},{})\n",
    "            train_edict[int(c1[0])][int(c1[3])][int(c1[1])] = float(c1[2])\n",
    "print len(train_edict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12845.0 19676.0 0.652825777597\n"
     ]
    }
   ],
   "source": [
    "tot = 0.\n",
    "corr = 0.\n",
    "src_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/src-train.txt', 'w')\n",
    "targ_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/targ-train.txt', 'w')\n",
    "src_ques_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/src-ques-train.txt', 'w')\n",
    "targ_ques_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/targ-ques-train.txt', 'w')\n",
    "label_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/label-train.txt', 'w')\n",
    "joint_writer = open(filepath + 'data/multirc/multirc-joint-train-nli.txt', 'w')\n",
    "for para in multirc_train:\n",
    "    for q_num1, ques in enumerate(para[1]):\n",
    "        answers = ques[1]\n",
    "        for i, a1 in enumerate(answers):\n",
    "            try:\n",
    "                prem_l = a1[1]\n",
    "                prem_id = train_id_dict[a1[2]]\n",
    "                prem_id_orig = a1[2]\n",
    "                for q_num2, ques2 in enumerate(para[1]):\n",
    "                    answers2 = ques2[1]\n",
    "                    for j, a2 in enumerate(answers2):\n",
    "                        if i != j and q_num1 == q_num2:\n",
    "                            hyp_l = a2[1]\n",
    "                            hyp_id = train_id_dict[a2[2]]\n",
    "                            hyp_id_orig = a2[2]\n",
    "\n",
    "                            #Writing to train file\n",
    "                            if prem_l == 'True':\n",
    "                                src_writer.write(unicode.encode(a1[0].text, errors='ignore') + '\\n')\n",
    "                                targ_writer.write(unicode.encode(a2[0].text, errors='ignore') + '\\n')\n",
    "                                src_ques_writer.write(unicode.encode(ques[0].text, errors='ignore') + '\\n')\n",
    "                                targ_ques_writer.write(unicode.encode(ques2[0].text, errors='ignore') + '\\n')\n",
    "                            else:\n",
    "                                pass\n",
    "                            if prem_l == 'True' and hyp_l == 'True':\n",
    "                                label_writer.write('entailment\\n')\n",
    "                                joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'entailment\\n')\n",
    "                            elif prem_l == 'True' and hyp_l == 'False':\n",
    "                                label_writer.write('contradiction\\n')\n",
    "                                joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'contradiction\\n')\n",
    "                            elif prem_l == 'False' and hyp_l == 'False':\n",
    "                                pass\n",
    "                                #joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'neutral\\n')\n",
    "                            elif prem_l == 'False' and hyp_l == 'True':\n",
    "                                pass\n",
    "                                #joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'neutral\\n')\n",
    "\n",
    "                            #Verifying effectiveness\n",
    "                            if prem_l == 'True':\n",
    "                                if hyp_id in train_edict[prem_id][0] and hyp_l == 'True':\n",
    "                                    tot += 1\n",
    "                                    corr += 1\n",
    "                                    pass\n",
    "                                elif hyp_id in train_edict[prem_id][2] and hyp_l == 'False':\n",
    "                                    tot += 1\n",
    "                                    corr += 1\n",
    "                                    pass\n",
    "                                elif hyp_id in train_edict[prem_id][2] or hyp_id in train_edict[prem_id][2]:\n",
    "                                    tot += 1\n",
    "                                    pass\n",
    "            except KeyError:\n",
    "                pass\n",
    "print corr, tot, corr / tot\n",
    "src_writer.close()\n",
    "targ_writer.close()\n",
    "src_ques_writer.close()\n",
    "targ_ques_writer.close()\n",
    "label_writer.close()\n",
    "joint_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22265\n"
     ]
    }
   ],
   "source": [
    "dev_edict = {}\n",
    "for l1 in open(filepath + 'results/multirc_train_nli', 'r').read().split('\\n'):\n",
    "    c1 = l1.split('\\t')\n",
    "    if len(c1) == 4:\n",
    "        try:\n",
    "            dev_edict[int(c1[0])][int(c1[3])][int(c1[1])] = float(c1[2])\n",
    "        except KeyError:\n",
    "            dev_edict[int(c1[0])] = ({},{},{})\n",
    "            dev_edict[int(c1[0])][int(c1[3])][int(c1[1])] = float(c1[2])\n",
    "print len(dev_edict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734.0 1227.0 0.598207008965\n"
     ]
    }
   ],
   "source": [
    "tot = 0.\n",
    "corr = 0.\n",
    "src_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/src-dev.txt', 'w')\n",
    "targ_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/targ-dev.txt', 'w')\n",
    "src_ques_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/src-ques-dev.txt', 'w')\n",
    "targ_ques_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/targ-ques-dev.txt', 'w')\n",
    "label_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/label-dev.txt', 'w')\n",
    "joint_writer = open(filepath + 'data/multirc/multirc-joint-dev-nli.txt', 'w')\n",
    "for para in multirc_dev:\n",
    "    for q_num1, ques in enumerate(para[1]):\n",
    "        answers = ques[1]\n",
    "        for i, a1 in enumerate(answers):\n",
    "            try:\n",
    "                prem_l = a1[1]\n",
    "                prem_id = dev_id_dict[a1[2]]\n",
    "                prem_id_orig = a1[2]\n",
    "                for q_num2, ques2 in enumerate(para[1]):\n",
    "                    answers2 = ques2[1]\n",
    "                    for j, a2 in enumerate(answers2):\n",
    "                        if i != j and q_num1 == q_num2:\n",
    "                            hyp_l = a2[1]\n",
    "                            hyp_id = dev_id_dict[a2[2]]\n",
    "                            hyp_id_orig = a2[2]\n",
    "\n",
    "                            #Writing to dev file\n",
    "                            if prem_l == 'True':\n",
    "                                src_writer.write(unicode.encode(a1[0].text, errors='ignore') + '\\n')\n",
    "                                targ_writer.write(unicode.encode(a2[0].text, errors='ignore') + '\\n')\n",
    "                                src_ques_writer.write(unicode.encode(ques[0].text, errors='ignore') + '\\n')\n",
    "                                targ_ques_writer.write(unicode.encode(ques2[0].text, errors='ignore') + '\\n')\n",
    "                            else:\n",
    "                                pass\n",
    "                            if prem_l == 'True' and hyp_l == 'True':\n",
    "                                label_writer.write('entailment\\n')\n",
    "                                joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'entailment\\n')\n",
    "                            elif prem_l == 'True' and hyp_l == 'False':\n",
    "                                label_writer.write('contradiction\\n')\n",
    "                                joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'contradiction\\n')\n",
    "                            elif prem_l == 'False' and hyp_l == 'False':\n",
    "                                pass\n",
    "                                #joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'neutral\\n')\n",
    "                            elif prem_l == 'False' and hyp_l == 'True':\n",
    "                                pass\n",
    "                                #joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'neutral\\n')   \n",
    "\n",
    "                            #Verifying effectiveness\n",
    "                            if prem_l == 'True':\n",
    "                                if hyp_id in dev_edict[prem_id][0] and hyp_l == 'True':\n",
    "                                    tot += 1\n",
    "                                    corr += 1\n",
    "                                    pass\n",
    "                                elif hyp_id in dev_edict[prem_id][2] and hyp_l == 'False':\n",
    "                                    tot += 1\n",
    "                                    corr += 1\n",
    "                                    pass\n",
    "                                elif hyp_id in dev_edict[prem_id][0] or hyp_id in dev_edict[prem_id][2]:\n",
    "                                    tot += 1\n",
    "                                    pass\n",
    "            except KeyError:\n",
    "                pass\n",
    "print corr, tot, corr / tot\n",
    "src_writer.close()\n",
    "targ_writer.close()\n",
    "src_ques_writer.close()\n",
    "targ_ques_writer.close()\n",
    "label_writer.close()\n",
    "joint_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/homes/rpujari/scratch/joint_model/results/nli', 'r') as infile:\n",
    "    with open(filepath + 'nli/preprocess/proc_multirc/multirc_joint_nli', 'w') as outfile:\n",
    "        for line in infile.read().split('\\n'):\n",
    "            cols = line.split('\\t')\n",
    "            if len(cols) == 4:\n",
    "                outfile.write(str(test_id_dict[cols[0]]) + '\\t' + str(test_id_dict[cols[1]]) + '\\t' + cols[3] + '\\t' + cols[2] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4848\n"
     ]
    }
   ],
   "source": [
    "test_edict = {}\n",
    "for l1 in open(filepath + 'results/multirc_allen_nli', 'r').read().split('\\n'):\n",
    "#for l1 in open(filepath + 'nli/preprocess/proc_multirc/dev_ent_res_st0_0.txt', 'r').read().split('\\n'):\n",
    "    c1 = l1.split('\\t')\n",
    "    if len(c1) == 4:\n",
    "        try:\n",
    "            test_edict[int(c1[0])][int(c1[3])][int(c1[1])] = float(c1[2])\n",
    "        except KeyError:\n",
    "            test_edict[int(c1[0])] = ({},{},{})\n",
    "            test_edict[int(c1[0])][int(c1[3])][int(c1[1])] = float(c1[2])\n",
    "print len(test_edict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3827.0 6944.0 0.551123271889\n"
     ]
    }
   ],
   "source": [
    "tot = 0.\n",
    "corr = 0.\n",
    "src_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/src-test.txt', 'w')\n",
    "targ_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/targ-test.txt', 'w')\n",
    "src_ques_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/src-ques-test.txt', 'w')\n",
    "targ_ques_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/targ-ques-test.txt', 'w')\n",
    "label_writer = open(filepath + 'nli/preprocess/proc_multirc/self_training/label-test.txt', 'w')\n",
    "joint_writer = open(filepath + 'data/multirc/multirc-joint-test-nli.txt', 'w')\n",
    "for para in multirc_test:\n",
    "    for q_num1, ques in enumerate(para[1]):\n",
    "        answers = ques[1]\n",
    "        for i, a1 in enumerate(answers):\n",
    "            try:\n",
    "                prem_l = a1[1]\n",
    "                prem_id = test_id_dict[a1[2]]\n",
    "                prem_id_orig = a1[2]\n",
    "                for q_num2, ques2 in enumerate(para[1]):\n",
    "                    answers2 = ques2[1]\n",
    "                    for j, a2 in enumerate(answers2):\n",
    "                        if i != j or q_num1 != q_num2:\n",
    "                            hyp_l = a2[1]\n",
    "                            hyp_id = test_id_dict[a2[2]]\n",
    "                            hyp_id_orig = a2[2]\n",
    "\n",
    "                            #Writing to test file\n",
    "                            if prem_l == 'True':\n",
    "                                src_writer.write(unicode.encode(a1[0].text, errors='ignore') + '\\n')\n",
    "                                targ_writer.write(unicode.encode(a2[0].text, errors='ignore') + '\\n')\n",
    "                                src_ques_writer.write(unicode.encode(ques[0].text, errors='ignore') + '\\n')\n",
    "                                targ_ques_writer.write(unicode.encode(ques2[0].text, errors='ignore') + '\\n')\n",
    "                            else:\n",
    "                                pass\n",
    "                            if prem_l == 'True' and hyp_l == 'True':\n",
    "                                label_writer.write('entailment\\n')\n",
    "                                joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'entailment\\n')\n",
    "                            elif prem_l == 'True' and hyp_l == 'False':\n",
    "                                label_writer.write('contradiction\\n')\n",
    "                                joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'contradiction\\n')\n",
    "                            elif prem_l == 'False' and hyp_l == 'False':\n",
    "                                joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'neutral\\n')\n",
    "                            elif prem_l == 'False' and hyp_l == 'True':\n",
    "                                joint_writer.write(prem_id_orig + '\\t' + hyp_id_orig + '\\t' + 'neutral\\n')   \n",
    "\n",
    "                            #Verifying effectiveness\n",
    "                            if prem_l == 'True':\n",
    "                                if hyp_id in test_edict[prem_id][0] and test_edict[prem_id][0][hyp_id] > 0.\\\n",
    "                                and hyp_l == 'True':\n",
    "                                    tot += 1\n",
    "                                    corr += 1\n",
    "                                    pass\n",
    "                                elif hyp_id in test_edict[prem_id][2] and test_edict[prem_id][2][hyp_id] > 0.\\\n",
    "                                and hyp_l == 'False':\n",
    "                                    tot += 1\n",
    "                                    corr += 1\n",
    "                                    pass\n",
    "                                elif (hyp_id in test_edict[prem_id][0] and test_edict[prem_id][0][hyp_id] > 0.)\\\n",
    "                                or (hyp_id in test_edict[prem_id][2] and test_edict[prem_id][2][hyp_id] > 0.):\n",
    "                                    tot += 1\n",
    "                                    pass\n",
    "            except KeyError:\n",
    "                pass\n",
    "print corr, tot, corr / tot\n",
    "src_writer.close()\n",
    "targ_writer.close()\n",
    "src_ques_writer.close()\n",
    "targ_ques_writer.close()\n",
    "label_writer.close()\n",
    "joint_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273, 3368)\n"
     ]
    }
   ],
   "source": [
    "ent_dict = {}\n",
    "contr_dict = {}\n",
    "with open(filepath + 'DRaiL/examples/multirc/test_entail.txt', 'w') as f1:\n",
    "    with open(filepath + 'DRaiL/examples/multirc/test_contradict.txt', 'w') as f2:\n",
    "        for prem_id in test_edict.keys():\n",
    "            e_d, n_d, c_d = test_edict[prem_id]\n",
    "            #c_d = test_allen_edict[prem_id][2]\n",
    "            for hyp_id in e_d.keys():\n",
    "                if e_d[hyp_id] >= 0.95:\n",
    "                #and test_labels[prem_id] == 'True' and test_labels[hyp_id] == 'True':\n",
    "                    f1.write(str(prem_id) + '\\t' + str(hyp_id) + '\\n')\n",
    "                    try:\n",
    "                        ent_dict[prem_id][hyp_id] = e_d[hyp_id]\n",
    "                    except KeyError:\n",
    "                        ent_dict[prem_id] = {}\n",
    "                        ent_dict[prem_id][hyp_id] = e_d[hyp_id]\n",
    "            for hyp_id in c_d.keys():\n",
    "                if c_d[hyp_id] >= 0.:\n",
    "                #and test_labels[prem_id] == 'True' and test_labels[hyp_id] == 'False':\n",
    "                    f2.write(str(prem_id) + '\\t' + str(hyp_id) + '\\n')\n",
    "                    try:\n",
    "                        contr_dict[prem_id][hyp_id] = c_d[hyp_id]\n",
    "                    except KeyError:\n",
    "                        contr_dict[prem_id] = {}\n",
    "                        contr_dict[prem_id][hyp_id] = c_d[hyp_id]\n",
    "        print(len(ent_dict), len(contr_dict))\n",
    "        \n",
    "        for para in multirc_test:\n",
    "            for qp in para[1]:\n",
    "                for i, a1 in enumerate(qp[1]):\n",
    "                    prem_id = test_id_dict[a1[2]]\n",
    "                    for j, a2 in enumerate(qp[1]):\n",
    "                        hyp_id = test_id_dict[a2[2]]\n",
    "                        if prem_id not in ent_dict.keys():\n",
    "                            ent_dict[prem_id] = {}\n",
    "                            if i == j:\n",
    "                                ent_dict[prem_id][hyp_id] = 1\n",
    "                            else:\n",
    "                                ent_dict[prem_id][hyp_id] = 0\n",
    "                        elif hyp_id not in ent_dict[prem_id].keys():\n",
    "                            if i == j:\n",
    "                                ent_dict[prem_id][hyp_id] = 1\n",
    "                            else:\n",
    "                                ent_dict[prem_id][hyp_id] = 0\n",
    "                        if prem_id not in contr_dict.keys():\n",
    "                            contr_dict[prem_id] = {}\n",
    "                            contr_dict[prem_id][hyp_id] = 0\n",
    "                        elif hyp_id not in contr_dict[prem_id].keys():\n",
    "                            contr_dict[prem_id][hyp_id] = 0\n",
    "        with open(filepath + 'DRaiL/examples/multirc/test_ent_probs.json', 'w') as f3:\n",
    "            f3.write(json.dumps(ent_dict))\n",
    "        with open(filepath + 'DRaiL/examples/multirc/test_contr_probs.json', 'w') as f4:\n",
    "            f4.write(json.dumps(contr_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Run inference.py on DRail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(filepath + 'DRaiL/inferred_res', 'r') as infile:\n",
    "    lines = infile.read().split('\\n')\n",
    "    res = [x.strip() for x in lines[13][6:-3].split(\"', '\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5193 6493\n"
     ]
    }
   ],
   "source": [
    "inf_res_list = [0] * 4848\n",
    "ents = set()\n",
    "contrs = set()\n",
    "for r_str in res:\n",
    "    if r_str.startswith('Has'):\n",
    "        inf_res_list[int(r_str.split(',')[0][9:])] = int(r_str.split(',')[1][:-1])\n",
    "    else:\n",
    "        if r_str.split('(')[0] == 'Entail':\n",
    "            ents.add(r_str.split('(')[1][:-1])\n",
    "        elif r_str.split('(')[0] == 'Contradict':\n",
    "            contrs.add(r_str.split('(')[1][:-1])\n",
    "\n",
    "print len(ents), len(contrs)\n",
    "for item in ents:\n",
    "    if item in contrs:\n",
    "        print item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2773.0, 2075.0)\n"
     ]
    }
   ],
   "source": [
    "gold_res = {}\n",
    "zeros = 0.\n",
    "ones = 0.\n",
    "with open('/homes/rpujari/scratch/raj_qa/results/yf_test_gold.old', 'r') as infile:\n",
    "    flines = infile.read().split('\\n')\n",
    "    for fline in flines:\n",
    "        cols = fline.split('\\t')\n",
    "        if len(cols) >= 2:\n",
    "            gold_res[cols[0].strip()] = int(cols[1])\n",
    "            if int(cols[1]) == 0:\n",
    "                zeros += 1\n",
    "            else:\n",
    "                ones += 1\n",
    "print(zeros, ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.697813531353\n"
     ]
    }
   ],
   "source": [
    "with open(filepath + 'tm_dev_res.json', 'w') as outfile:\n",
    "    p_num = 0\n",
    "    acc = 0.\n",
    "    test_res = []\n",
    "    for para in multirc_test:\n",
    "        q_num = 0\n",
    "        pid = multirc_test_orig['data'][p_num]['id']\n",
    "        for qp in para[1]:\n",
    "            q_dict = {}\n",
    "            q_dict['pid'] = pid\n",
    "            q_dict['qid'] = str(qp[1][0][2].split('_')[1])\n",
    "            answers = []\n",
    "            for ans in qp[1]:\n",
    "                answers.append(inf_res_list[test_id_dict[ans[2]]])\n",
    "                if inf_res_list[test_id_dict[ans[2]]] == gold_res[ans[2]]:\n",
    "                    acc += 1\n",
    "            q_dict['scores'] = answers\n",
    "            q_num += 1\n",
    "            test_res.append(q_dict)\n",
    "        p_num += 1\n",
    "    outfile.write(json.dumps(test_res))\n",
    "    print(acc / len(inf_res_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
